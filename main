import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader import pandas as pd import os import spacy # Tokenizacja za pomocą spaCy nlp = spacy.load("pl_core_news_sm") # Klasa reprezentująca model class MathModel(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MathModel, self).__init__() self.embedding = nn.Embedding(input_size, hidden_size) self.gru1 = nn.GRU(hidden_size, hidden_size, batch_first=True) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x): embedded = self.embedding(x) output, _ = self.gru1(embedded) output = self.fc(output[:, -1, :]) # Weź ostatnią warstwę z sekwencji return output # Klasa reprezentująca zbiór danych class MathDataset(Dataset): def __init__(self, questions, answers, word2index): self.questions = questions self.answers = answers self.word2index = word2index def __len__(self): return len(self.questions) def __getitem__(self, idx): question = self.questions[idx] # Tokenizacja za pomocą spaCy question_words = [word.text.lower() for word in nlp(question)] if not any(word in self.word2index for word in question_words): # Zwróć etykietę -1 dla pytania bez znanych słów return torch.tensor([-1], dtype=torch.long), -1 question_tensor = torch.tensor([self.word2index.get(word, 0) for word in question_words], dtype=torch.long) answer = self.answers[idx] return question_tensor, answer # Funkcja do trenowania modelu def train_model(model, data_loader, criterion, optimizer, word2index, num_epochs=10): for epoch in range(num_epochs): total_loss = 0.0 for question_tensor, answer in data_loader: if (question_tensor == -1).any(): continue # Skip the iteration if question_tensor is -1 optimizer.zero_grad() output = model(question_tensor) target = torch.tensor([answer], dtype=torch.long) loss = criterion(output, target) loss.backward() optimizer.step() total_loss += loss.item() print(f'Epocha [{epoch + 1}/{num_epochs}], Strata: {total_loss / len(data_loader)}') # Funkcja do zapisywania modelu def save_model(model, file_path): torch.save(model.state_dict(), file_path) print(f"Model saved to {file_path}") # Funkcja do ładowania modelu def load_model(model, file_path): model.load_state_dict(torch.load(file_path)) print(f"Model loaded from {file_path}") return model # Funkcja do testowania modelu def test_model(model, question, word2index): model.eval() with torch.no_grad(): question_tensor = torch.tensor([word2index.get(word, 0) for word in question], dtype=torch.long).unsqueeze(0) # W powyższym fragmencie word2index.get(word, 0) zwraca 0, jeśli słowo nie istnieje w słowniku output = model(question_tensor) _, predicted_class = torch.max(output, 1) return predicted_class[0].item() # Funkcja do ładowania danych def load_data(file_path): data = pd.read_csv(file_path) # Usuń wiersze zawierające wartości NaN w kolumnach 'Pytanie' i 'Odpowiedz' data = data.dropna(subset=['Pytanie', 'Odpowiedz']) if 'Pytanie' in data.columns and 'Odpowiedz' in data.columns: questions = data['Pytanie'].astype(str).tolist() # Zmiana na typ str answers = data['Odpowiedz'].astype(str).tolist() # Tokenizacja za pomocą spaCy questions_tokenized = [nlp(question) for question in questions] unique_labels = sorted(set(answers)) label2index = {label: i for i, label in enumerate(unique_labels)} answers = [label2index[label] for label in answers] return questions_tokenized, answers # Funkcja do menu głównego def main(): data_file = "TestData.csv" model_file = "math_model.pth" # Inicjalizacja słownika word2index word2index = {word.text.lower(): i for i, word in enumerate(nlp("0123456789()+-*/^ "))} # Sprawdź, czy model już istnieje model_exists = os.path.exists(model_file) # Jeśli model już istnieje, wczytaj go if model_exists: questions, answers = load_data(data_file) model = MathModel(len(word2index), 512, len(set(answers))) # Zwiększenie rozmiaru sieci model.load_state_dict(torch.load(model_file), strict=False) print("Model istnieje. Możesz korzystać z opcji 1.") else: # Wczytaj dane i uzyskaj odpowiedzi przed inicjalizacją modelu questions, answers = load_data(data_file) model = MathModel(len(word2index), 512, len(set(answers))) # Zwiększenie rozmiaru sieci print("Model nie istnieje. Należy nauczyć model, korzystając z opcji 2.") while True: print("\nMenu:") print("1. Korzystaj z modelem") print("2. Ucz model") print("3. Zapisz model") print("4. Wczytaj inny model") print("5. Wyjście") choice = input("Wybierz opcję (1-5): ") if choice == '1': if model_exists: while True: question = input("Zadaj pytanie w języku polskim (lub wpisz 'exit' aby wyjść): ") if question.lower() == 'exit': break predicted_class = test_model(model, question, word2index) print(f"Przewidziana odpowiedź: {predicted_class}") else: print("Model nie istnieje. Wybierz opcję 2, aby go nauczyć.") elif choice == '2': math_dataset = MathDataset(questions, answers, word2index) data_loader = DataLoader(math_dataset, batch_size=1, shuffle=True) criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) num_epochs = int(input("Podaj liczbę epok do nauki: ")) train_model(model, data_loader, criterion, optimizer, word2index, num_epochs) model_exists = True elif choice == '3': if model_exists: save_model(model, model_file) else: print("Model nie istnieje. Wybierz opcję 2, aby go nauczyć.") elif choice == '4': new_model_file = input("Podaj nazwę pliku z modelem do wczytania (np. inny_model.pth): ") model = load_model(model, new_model_file) model_exists = True elif choice == '5': break else: print("Nieprawidłowy wybór. Wybierz liczbę od 1 do 5.") if __name__ == "__main__": main()